{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from torchtext.vocab import GloVe\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# business = []\n",
    "# with open(\"../dataset/business_all.json\", \"r\") as file:\n",
    "#     for line in file:\n",
    "#         business.append(json.loads(line))\n",
    "\n",
    "# checkin = []\n",
    "# with open(\"../dataset/checkin_all.json\", \"r\") as file:\n",
    "#     for line in file:\n",
    "#         checkin.append(json.loads(line))\n",
    "\n",
    "# review = []\n",
    "# with open(\"../dataset/review_all.json\", \"r\") as file:\n",
    "#     for line in file:\n",
    "#         review.append(json.loads(line))\n",
    "\n",
    "# user = []\n",
    "# with open(\"../dataset/user_all.json\", \"r\") as file:\n",
    "#     for line in file:\n",
    "#         user.append(json.loads(line))\n",
    "\n",
    "\n",
    "review_train = []\n",
    "with open(\"../dataset/review_train.json\", \"r\") as file:\n",
    "    for line in file:\n",
    "        review_train.append(json.loads(line))\n",
    "\n",
    "review_test = []\n",
    "with open(\"../dataset/review_test.json\", \"r\") as file:\n",
    "    for line in file:\n",
    "        review_test.append(json.loads(line))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrible place to shop!!! The manager is an expletive i cannot say on here. I went and bought several nails and screws as well as a rug and other items. While i was checking out the cashier charged me incorrectly for some of the screws, actually charged me more than what they were so i told the cashier and she corrected her mistake. While shopping i was there for about an hour because i was looking at paint, i even talked to a guy while i was there about different paint options as well as another lady about drill bits and about my cat condo project. anyway after i left i realize after i got home the cashier had also forgotten to give me a receipt. the rug i purchased did not work where i wanted it so i attempted to return it about 1 week after i purchased it, i went back the following weekend and the manager refused. i have never heard of refusing to return something or even give a store credit. im not one to return stuff so i was completely shocked. when i put up a stink with the manager she absolutely rude. i will never go there and the reason is that this location along with several others are individually owned and because of this they can make up whatever policies they want. so back to Hd or Lowes i go. never again!!\n"
     ]
    }
   ],
   "source": [
    "print(review_train[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBOW2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(x) * 4 + 1\n",
    "        return x\n",
    "\n",
    "class NNBOW3(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x) * 4 + 1\n",
    "        return x\n",
    "\n",
    "class NNBOW4(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.fc4 = nn.Linear(hidden_size//2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/outakaratakara/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/outakaratakara/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review_text(review_text):\n",
    "    review_text = review_text.lower()\n",
    "    review_text = re.sub(r'\\W+', ' ', review_text)\n",
    "    words = [word for word in review_text.split() if word not in stop_words]\n",
    "    processed_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    return processed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_text_train = [\n",
    "#     re.sub(r'\\W+', ' ', review['text'].lower()).split()\n",
    "#     for review in review_train\n",
    "# ]\n",
    "\n",
    "# review_text_test = [\n",
    "#     re.sub(r'\\W+', ' ', review['text'].lower()).split()\n",
    "#     for review in review_test\n",
    "# ]\n",
    "\n",
    "review_text_train = [\n",
    "    preprocess_review_text(review['text']) for review in review_train\n",
    "]\n",
    "\n",
    "review_text_test = [\n",
    "    preprocess_review_text(review['text']) for review in review_test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_train = [review['stars'] for review in review_train]\n",
    "rating_test = [review['stars'] for review in review_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=512)\n",
    "\n",
    "review_text_sentences_train = [\" \".join(words) for words in review_text_train]\n",
    "bow_embeddings_train = (vectorizer.fit_transform(review_text_sentences_train)).toarray()\n",
    "\n",
    "review_text_sentences_test = [\" \".join(words) for words in review_text_test]\n",
    "bow_embeddings_test = (vectorizer.fit_transform(review_text_sentences_test)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    bow_embeddings_train,  \n",
    "    rating_train,                 \n",
    "    test_size=0.2,              \n",
    "    random_state=42               \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  \n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)   \n",
    "X_test_tensor = torch.tensor(bow_embeddings_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(rating_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 128 \n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        predictions = model(X_train)\n",
    "        loss = criterion(predictions, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(X_val)\n",
    "            val_loss = criterion(val_predictions, y_val)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = model(X_test)\n",
    "        test_loss = criterion(test_predictions, y_test)\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/140, Loss: 1.9780, Val Loss: 1.7876\n",
      "Epoch 2/140, Loss: 1.7808, Val Loss: 1.7088\n",
      "Epoch 3/140, Loss: 1.7024, Val Loss: 1.6513\n",
      "Epoch 4/140, Loss: 1.6442, Val Loss: 1.5740\n",
      "Epoch 5/140, Loss: 1.5654, Val Loss: 1.4820\n",
      "Epoch 6/140, Loss: 1.4711, Val Loss: 1.4019\n",
      "Epoch 7/140, Loss: 1.3891, Val Loss: 1.3571\n",
      "Epoch 8/140, Loss: 1.3438, Val Loss: 1.3319\n",
      "Epoch 9/140, Loss: 1.3182, Val Loss: 1.2993\n",
      "Epoch 10/140, Loss: 1.2852, Val Loss: 1.2592\n",
      "Epoch 11/140, Loss: 1.2445, Val Loss: 1.2233\n",
      "Epoch 12/140, Loss: 1.2082, Val Loss: 1.1989\n",
      "Epoch 13/140, Loss: 1.1832, Val Loss: 1.1838\n",
      "Epoch 14/140, Loss: 1.1676, Val Loss: 1.1716\n",
      "Epoch 15/140, Loss: 1.1549, Val Loss: 1.1576\n",
      "Epoch 16/140, Loss: 1.1404, Val Loss: 1.1412\n",
      "Epoch 17/140, Loss: 1.1236, Val Loss: 1.1248\n",
      "Epoch 18/140, Loss: 1.1068, Val Loss: 1.1111\n",
      "Epoch 19/140, Loss: 1.0928, Val Loss: 1.1013\n",
      "Epoch 20/140, Loss: 1.0824, Val Loss: 1.0941\n",
      "Epoch 21/140, Loss: 1.0749, Val Loss: 1.0879\n",
      "Epoch 22/140, Loss: 1.0683, Val Loss: 1.0812\n",
      "Epoch 23/140, Loss: 1.0613, Val Loss: 1.0737\n",
      "Epoch 24/140, Loss: 1.0535, Val Loss: 1.0661\n",
      "Epoch 25/140, Loss: 1.0455, Val Loss: 1.0593\n",
      "Epoch 26/140, Loss: 1.0383, Val Loss: 1.0538\n",
      "Epoch 27/140, Loss: 1.0325, Val Loss: 1.0495\n",
      "Epoch 28/140, Loss: 1.0279, Val Loss: 1.0459\n",
      "Epoch 29/140, Loss: 1.0239, Val Loss: 1.0423\n",
      "Epoch 30/140, Loss: 1.0201, Val Loss: 1.0384\n",
      "Epoch 31/140, Loss: 1.0160, Val Loss: 1.0343\n",
      "Epoch 32/140, Loss: 1.0117, Val Loss: 1.0303\n",
      "Epoch 33/140, Loss: 1.0075, Val Loss: 1.0267\n",
      "Epoch 34/140, Loss: 1.0038, Val Loss: 1.0238\n",
      "Epoch 35/140, Loss: 1.0007, Val Loss: 1.0213\n",
      "Epoch 36/140, Loss: 0.9980, Val Loss: 1.0190\n",
      "Epoch 37/140, Loss: 0.9956, Val Loss: 1.0168\n",
      "Epoch 38/140, Loss: 0.9932, Val Loss: 1.0144\n",
      "Epoch 39/140, Loss: 0.9906, Val Loss: 1.0121\n",
      "Epoch 40/140, Loss: 0.9881, Val Loss: 1.0099\n",
      "Epoch 41/140, Loss: 0.9856, Val Loss: 1.0079\n",
      "Epoch 42/140, Loss: 0.9835, Val Loss: 1.0062\n",
      "Epoch 43/140, Loss: 0.9815, Val Loss: 1.0047\n",
      "Epoch 44/140, Loss: 0.9798, Val Loss: 1.0031\n",
      "Epoch 45/140, Loss: 0.9781, Val Loss: 1.0015\n",
      "Epoch 46/140, Loss: 0.9764, Val Loss: 0.9998\n",
      "Epoch 47/140, Loss: 0.9746, Val Loss: 0.9982\n",
      "Epoch 48/140, Loss: 0.9729, Val Loss: 0.9966\n",
      "Epoch 49/140, Loss: 0.9713, Val Loss: 0.9951\n",
      "Epoch 50/140, Loss: 0.9699, Val Loss: 0.9938\n",
      "Epoch 51/140, Loss: 0.9685, Val Loss: 0.9925\n",
      "Epoch 52/140, Loss: 0.9672, Val Loss: 0.9913\n",
      "Epoch 53/140, Loss: 0.9659, Val Loss: 0.9901\n",
      "Epoch 54/140, Loss: 0.9647, Val Loss: 0.9890\n",
      "Epoch 55/140, Loss: 0.9634, Val Loss: 0.9879\n",
      "Epoch 56/140, Loss: 0.9622, Val Loss: 0.9869\n",
      "Epoch 57/140, Loss: 0.9611, Val Loss: 0.9859\n",
      "Epoch 58/140, Loss: 0.9601, Val Loss: 0.9850\n",
      "Epoch 59/140, Loss: 0.9591, Val Loss: 0.9841\n",
      "Epoch 60/140, Loss: 0.9581, Val Loss: 0.9832\n",
      "Epoch 61/140, Loss: 0.9572, Val Loss: 0.9823\n",
      "Epoch 62/140, Loss: 0.9563, Val Loss: 0.9815\n",
      "Epoch 63/140, Loss: 0.9554, Val Loss: 0.9807\n",
      "Epoch 64/140, Loss: 0.9546, Val Loss: 0.9799\n",
      "Epoch 65/140, Loss: 0.9538, Val Loss: 0.9792\n",
      "Epoch 66/140, Loss: 0.9530, Val Loss: 0.9785\n",
      "Epoch 67/140, Loss: 0.9523, Val Loss: 0.9778\n",
      "Epoch 68/140, Loss: 0.9516, Val Loss: 0.9771\n",
      "Epoch 69/140, Loss: 0.9509, Val Loss: 0.9765\n",
      "Epoch 70/140, Loss: 0.9502, Val Loss: 0.9759\n",
      "Epoch 71/140, Loss: 0.9496, Val Loss: 0.9754\n",
      "Epoch 72/140, Loss: 0.9490, Val Loss: 0.9748\n",
      "Epoch 73/140, Loss: 0.9484, Val Loss: 0.9743\n",
      "Epoch 74/140, Loss: 0.9479, Val Loss: 0.9737\n",
      "Epoch 75/140, Loss: 0.9473, Val Loss: 0.9732\n",
      "Epoch 76/140, Loss: 0.9468, Val Loss: 0.9727\n",
      "Epoch 77/140, Loss: 0.9463, Val Loss: 0.9723\n",
      "Epoch 78/140, Loss: 0.9458, Val Loss: 0.9718\n",
      "Epoch 79/140, Loss: 0.9453, Val Loss: 0.9714\n",
      "Epoch 80/140, Loss: 0.9449, Val Loss: 0.9710\n",
      "Epoch 81/140, Loss: 0.9444, Val Loss: 0.9706\n",
      "Epoch 82/140, Loss: 0.9440, Val Loss: 0.9702\n",
      "Epoch 83/140, Loss: 0.9436, Val Loss: 0.9698\n",
      "Epoch 84/140, Loss: 0.9432, Val Loss: 0.9695\n",
      "Epoch 85/140, Loss: 0.9428, Val Loss: 0.9692\n",
      "Epoch 86/140, Loss: 0.9425, Val Loss: 0.9688\n",
      "Epoch 87/140, Loss: 0.9421, Val Loss: 0.9685\n",
      "Epoch 88/140, Loss: 0.9418, Val Loss: 0.9682\n",
      "Epoch 89/140, Loss: 0.9414, Val Loss: 0.9679\n",
      "Epoch 90/140, Loss: 0.9411, Val Loss: 0.9676\n",
      "Epoch 91/140, Loss: 0.9408, Val Loss: 0.9673\n",
      "Epoch 92/140, Loss: 0.9405, Val Loss: 0.9670\n",
      "Epoch 93/140, Loss: 0.9402, Val Loss: 0.9668\n",
      "Epoch 94/140, Loss: 0.9399, Val Loss: 0.9665\n",
      "Epoch 95/140, Loss: 0.9397, Val Loss: 0.9663\n",
      "Epoch 96/140, Loss: 0.9394, Val Loss: 0.9660\n",
      "Epoch 97/140, Loss: 0.9391, Val Loss: 0.9658\n",
      "Epoch 98/140, Loss: 0.9389, Val Loss: 0.9656\n",
      "Epoch 99/140, Loss: 0.9387, Val Loss: 0.9653\n",
      "Epoch 100/140, Loss: 0.9384, Val Loss: 0.9651\n",
      "Epoch 101/140, Loss: 0.9382, Val Loss: 0.9649\n",
      "Epoch 102/140, Loss: 0.9380, Val Loss: 0.9647\n",
      "Epoch 103/140, Loss: 0.9378, Val Loss: 0.9645\n",
      "Epoch 104/140, Loss: 0.9376, Val Loss: 0.9643\n",
      "Epoch 105/140, Loss: 0.9374, Val Loss: 0.9641\n",
      "Epoch 106/140, Loss: 0.9372, Val Loss: 0.9639\n",
      "Epoch 107/140, Loss: 0.9370, Val Loss: 0.9638\n",
      "Epoch 108/140, Loss: 0.9368, Val Loss: 0.9636\n",
      "Epoch 109/140, Loss: 0.9367, Val Loss: 0.9634\n",
      "Epoch 110/140, Loss: 0.9365, Val Loss: 0.9633\n",
      "Epoch 111/140, Loss: 0.9363, Val Loss: 0.9631\n",
      "Epoch 112/140, Loss: 0.9362, Val Loss: 0.9630\n",
      "Epoch 113/140, Loss: 0.9360, Val Loss: 0.9628\n",
      "Epoch 114/140, Loss: 0.9359, Val Loss: 0.9627\n",
      "Epoch 115/140, Loss: 0.9357, Val Loss: 0.9626\n",
      "Epoch 116/140, Loss: 0.9356, Val Loss: 0.9624\n",
      "Epoch 117/140, Loss: 0.9354, Val Loss: 0.9623\n",
      "Epoch 118/140, Loss: 0.9353, Val Loss: 0.9622\n",
      "Epoch 119/140, Loss: 0.9352, Val Loss: 0.9621\n",
      "Epoch 120/140, Loss: 0.9351, Val Loss: 0.9620\n",
      "Epoch 121/140, Loss: 0.9349, Val Loss: 0.9618\n",
      "Epoch 122/140, Loss: 0.9348, Val Loss: 0.9617\n",
      "Epoch 123/140, Loss: 0.9347, Val Loss: 0.9616\n",
      "Epoch 124/140, Loss: 0.9346, Val Loss: 0.9615\n",
      "Epoch 125/140, Loss: 0.9345, Val Loss: 0.9614\n",
      "Epoch 126/140, Loss: 0.9344, Val Loss: 0.9614\n",
      "Epoch 127/140, Loss: 0.9343, Val Loss: 0.9613\n",
      "Epoch 128/140, Loss: 0.9342, Val Loss: 0.9612\n",
      "Epoch 129/140, Loss: 0.9341, Val Loss: 0.9611\n",
      "Epoch 130/140, Loss: 0.9340, Val Loss: 0.9610\n",
      "Epoch 131/140, Loss: 0.9339, Val Loss: 0.9609\n",
      "Epoch 132/140, Loss: 0.9339, Val Loss: 0.9609\n",
      "Epoch 133/140, Loss: 0.9338, Val Loss: 0.9608\n",
      "Epoch 134/140, Loss: 0.9337, Val Loss: 0.9607\n",
      "Epoch 135/140, Loss: 0.9336, Val Loss: 0.9607\n",
      "Epoch 136/140, Loss: 0.9336, Val Loss: 0.9606\n",
      "Epoch 137/140, Loss: 0.9335, Val Loss: 0.9605\n",
      "Epoch 138/140, Loss: 0.9334, Val Loss: 0.9605\n",
      "Epoch 139/140, Loss: 0.9333, Val Loss: 0.9604\n",
      "Epoch 140/140, Loss: 0.9333, Val Loss: 0.9603\n"
     ]
    }
   ],
   "source": [
    "model_2 = NNBOW2(input_size, hidden_size)\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr=0.01)\n",
    "\n",
    "train_model(\n",
    "    model=model_2,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_val=X_val_tensor,\n",
    "    y_val=y_val_tensor,\n",
    "    num_epochs=140\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60, Loss: 2.0656, Val Loss: 1.8502\n",
      "Epoch 2/60, Loss: 1.8482, Val Loss: 1.5998\n",
      "Epoch 3/60, Loss: 1.5936, Val Loss: 1.5339\n",
      "Epoch 4/60, Loss: 1.5246, Val Loss: 1.3668\n",
      "Epoch 5/60, Loss: 1.3557, Val Loss: 1.2380\n",
      "Epoch 6/60, Loss: 1.2252, Val Loss: 1.1568\n",
      "Epoch 7/60, Loss: 1.1428, Val Loss: 1.0818\n",
      "Epoch 8/60, Loss: 1.0678, Val Loss: 1.0731\n",
      "Epoch 9/60, Loss: 1.0578, Val Loss: 1.0593\n",
      "Epoch 10/60, Loss: 1.0424, Val Loss: 1.0361\n",
      "Epoch 11/60, Loss: 1.0175, Val Loss: 1.0300\n",
      "Epoch 12/60, Loss: 1.0098, Val Loss: 1.0042\n",
      "Epoch 13/60, Loss: 0.9825, Val Loss: 0.9732\n",
      "Epoch 14/60, Loss: 0.9503, Val Loss: 0.9532\n",
      "Epoch 15/60, Loss: 0.9300, Val Loss: 0.9220\n",
      "Epoch 16/60, Loss: 0.8987, Val Loss: 0.9119\n",
      "Epoch 17/60, Loss: 0.8883, Val Loss: 0.8902\n",
      "Epoch 18/60, Loss: 0.8667, Val Loss: 0.8865\n",
      "Epoch 19/60, Loss: 0.8635, Val Loss: 0.8718\n",
      "Epoch 20/60, Loss: 0.8480, Val Loss: 0.8721\n",
      "Epoch 21/60, Loss: 0.8468, Val Loss: 0.8598\n",
      "Epoch 22/60, Loss: 0.8342, Val Loss: 0.8581\n",
      "Epoch 23/60, Loss: 0.8328, Val Loss: 0.8479\n",
      "Epoch 24/60, Loss: 0.8207, Val Loss: 0.8486\n",
      "Epoch 25/60, Loss: 0.8191, Val Loss: 0.8375\n",
      "Epoch 26/60, Loss: 0.8078, Val Loss: 0.8357\n",
      "Epoch 27/60, Loss: 0.8057, Val Loss: 0.8295\n",
      "Epoch 28/60, Loss: 0.7972, Val Loss: 0.8306\n",
      "Epoch 29/60, Loss: 0.7963, Val Loss: 0.8230\n",
      "Epoch 30/60, Loss: 0.7888, Val Loss: 0.8213\n",
      "Epoch 31/60, Loss: 0.7871, Val Loss: 0.8165\n",
      "Epoch 32/60, Loss: 0.7808, Val Loss: 0.8150\n",
      "Epoch 33/60, Loss: 0.7783, Val Loss: 0.8092\n",
      "Epoch 34/60, Loss: 0.7725, Val Loss: 0.8068\n",
      "Epoch 35/60, Loss: 0.7693, Val Loss: 0.8040\n",
      "Epoch 36/60, Loss: 0.7642, Val Loss: 0.8015\n",
      "Epoch 37/60, Loss: 0.7599, Val Loss: 0.7989\n",
      "Epoch 38/60, Loss: 0.7561, Val Loss: 0.7962\n",
      "Epoch 39/60, Loss: 0.7512, Val Loss: 0.7956\n",
      "Epoch 40/60, Loss: 0.7483, Val Loss: 0.7910\n",
      "Epoch 41/60, Loss: 0.7427, Val Loss: 0.7886\n",
      "Epoch 42/60, Loss: 0.7388, Val Loss: 0.7879\n",
      "Epoch 43/60, Loss: 0.7354, Val Loss: 0.7844\n",
      "Epoch 44/60, Loss: 0.7304, Val Loss: 0.7834\n",
      "Epoch 45/60, Loss: 0.7269, Val Loss: 0.7842\n",
      "Epoch 46/60, Loss: 0.7238, Val Loss: 0.7818\n",
      "Epoch 47/60, Loss: 0.7194, Val Loss: 0.7806\n",
      "Epoch 48/60, Loss: 0.7149, Val Loss: 0.7801\n",
      "Epoch 49/60, Loss: 0.7115, Val Loss: 0.7791\n",
      "Epoch 50/60, Loss: 0.7087, Val Loss: 0.7803\n",
      "Epoch 51/60, Loss: 0.7060, Val Loss: 0.7790\n",
      "Epoch 52/60, Loss: 0.7028, Val Loss: 0.7800\n",
      "Epoch 53/60, Loss: 0.6996, Val Loss: 0.7785\n",
      "Epoch 54/60, Loss: 0.6957, Val Loss: 0.7784\n",
      "Epoch 55/60, Loss: 0.6918, Val Loss: 0.7770\n",
      "Epoch 56/60, Loss: 0.6878, Val Loss: 0.7767\n",
      "Epoch 57/60, Loss: 0.6841, Val Loss: 0.7764\n",
      "Epoch 58/60, Loss: 0.6806, Val Loss: 0.7767\n",
      "Epoch 59/60, Loss: 0.6774, Val Loss: 0.7773\n",
      "Epoch 60/60, Loss: 0.6743, Val Loss: 0.7780\n"
     ]
    }
   ],
   "source": [
    "model_3 = NNBOW3(input_size, hidden_size)\n",
    "\n",
    "optimizer_3 = torch.optim.Adam(model_3.parameters(), lr=0.01)\n",
    "train_model(\n",
    "    model=model_3,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer_3,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_val=X_val_tensor,\n",
    "    y_val=y_val_tensor,\n",
    "    num_epochs=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3.6611, 3.6580, 3.6729,  ..., 3.6492, 3.6616, 3.6763],\n",
       "        [4.1086, 4.1287, 4.1296,  ..., 4.1179, 4.1237, 4.1064],\n",
       "        [3.3301, 3.3489, 3.3594,  ..., 3.3276, 3.3453, 3.3473],\n",
       "        ...,\n",
       "        [4.4614, 4.4703, 4.4555,  ..., 4.4748, 4.4638, 4.4493],\n",
       "        [4.1353, 4.1184, 4.1367,  ..., 4.1181, 4.1234, 4.1167],\n",
       "        [4.2338, 4.2453, 4.2427,  ..., 4.2356, 4.2406, 4.2351]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(\n",
    "    model=model_2,\n",
    "    criterion=criterion,\n",
    "    X_test=X_test_tensor,\n",
    "    y_test=y_test_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.0311],\n",
       "        [3.7657],\n",
       "        [3.0412],\n",
       "        ...,\n",
       "        [3.7832],\n",
       "        [4.3319],\n",
       "        [4.1538]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(\n",
    "    model=model_3,\n",
    "    criterion=criterion,\n",
    "    X_test=X_test_tensor,\n",
    "    y_test=y_test_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
